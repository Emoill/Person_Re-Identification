{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Lambda, Dense, MaxPooling2D, Input, Flatten, Dropout, BatchNormalization, LeakyReLU, Activation, Add\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_number(filename):\n",
    "    # this function extracts the first sequence of digits from the image name. I use it to identify the same people later.\n",
    "    match = re.search(r'\\d+', filename)\n",
    "\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def resize_and_pad_image(img, target_size=(64, 64)):\n",
    "    # this function resizes the image to the target size and pads it with zeros to keep the aspect ratio\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    target_height, target_width = target_size\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    if width == height:\n",
    "        return cv2.resize(img, target_size)\n",
    "\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / aspect_ratio)\n",
    "    else:\n",
    "        new_width = int(target_height * aspect_ratio)\n",
    "        new_height = target_height\n",
    "\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    top_pad = (target_height - new_height) // 2\n",
    "    bottom_pad = target_height - new_height - top_pad\n",
    "    left_pad = (target_width - new_width) // 2\n",
    "    right_pad = target_width - new_width - left_pad\n",
    "\n",
    "    padded_img = cv2.copyMakeBorder(resized_img, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = '../Market-1501-v15.09.15/bounding_box_train/'):\n",
    "    # this function loads the Images from the folder and save there labels\n",
    "    files = os.listdir(path)\n",
    "    files.sort()\n",
    "    data = []\n",
    "    counter = 0\n",
    "    count_per_person = {}\n",
    "    label = []\n",
    "    for file in files:\n",
    "            person_id = file.split('_')[0]\n",
    "\n",
    "            if person_id not in count_per_person:\n",
    "                count_per_person[person_id] = 1\n",
    "            elif count_per_person[person_id] < 30:\n",
    "                count_per_person[person_id] += 1\n",
    "            else:\n",
    "                continue  \n",
    "            img = cv2.imread(path + file)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img/255.0\n",
    "\n",
    "            img_resized = resize_and_pad_image(img, target_size=(64, 64))\n",
    "\n",
    "            data.append(img_resized)\n",
    "            label.append(extract_first_number(file))\n",
    "\n",
    "            counter += 1\n",
    "            if counter%1000 == 0:\n",
    "              print(\"Verarbeitung von 1000 Bildern abgeschlossen.\")\n",
    "            if counter == 3000:\n",
    "                break\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeitung von 1000 Bildern abgeschlossen.\n",
      "Verarbeitung von 1000 Bildern abgeschlossen.\n",
      "Verarbeitung von 1000 Bildern abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "images, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_pairs(images, labels, num_positive_pairs=1000, num_negative_pairs=1000):\n",
    "    pairs = []\n",
    "    labels = np.array(labels)\n",
    "    labels_a = []\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    for _ in range(num_positive_pairs):\n",
    "        person_label = np.random.choice(unique_labels)\n",
    "\n",
    "        pair_indices = np.random.choice(np.where(labels == person_label)[0], 2, replace=False)\n",
    "\n",
    "        pairs.append([images[pair_indices[0]], images[pair_indices[1]]])\n",
    "        labels = np.append(labels, 1)\n",
    "\n",
    "    for _ in range(num_negative_pairs):\n",
    "        person1, person2 = np.random.choice(unique_labels, 2, replace=False)\n",
    "\n",
    "        indices1 = np.random.choice(np.where(labels == person1)[0])\n",
    "        indices2 = np.random.choice(np.where(labels == person2)[0])\n",
    "        \n",
    "        pairs.append([images[indices1], images[indices2]])\n",
    "        labels = np.append(labels, 0)\n",
    "\n",
    "    return np.array(pairs), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = create_random_pairs(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_labels = labels[-2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the custom Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_network(input_shape):\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(256, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "    encoded_a = model(input_a)\n",
    "    encoded_b = model(input_b)\n",
    "\n",
    "    l1_distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([encoded_a, encoded_b])\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid')(l1_distance)\n",
    "\n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 1024)                 3328128   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 1024)                 0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    1025      ['lambda[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3329153 (12.70 MB)\n",
      "Trainable params: 3325697 (12.69 MB)\n",
      "Non-trainable params: 3456 (13.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (64, 64, 3)\n",
    "siamese_model = create_siamese_network(input_shape)\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.load_weights('./siamese_model_weights_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=0.001),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 9s 132ms/step - loss: 0.4480 - accuracy: 0.8940\n",
      "Validation Loss: 0.4479917883872986, Validation Accuracy: 0.8939999938011169\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_accuracy = siamese_model.evaluate([pairs[:, 0], pairs[:, 1]], pair_labels)\n",
    "print(f'Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Siamese Network to TF-Lite\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Emil\\AppData\\Local\\Temp\\tmp7u1cm2e_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Emil\\AppData\\Local\\Temp\\tmp7u1cm2e_\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('siamese_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eingabe-Tensoren:\n",
      "[{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 64, 64,  3]), 'shape_signature': array([-1, 64, 64,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_2:0', 'index': 1, 'shape': array([ 1, 64, 64,  3]), 'shape_signature': array([-1, 64, 64,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "Ausgabe-Tensoren:\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 48, 'shape': array([1, 1]), 'shape_signature': array([-1,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tflite_model_path = './Model/siamese_model.tflite'\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Eingabe-Tensoren:\")\n",
    "print(input_details)\n",
    "print(\"\\nAusgabe-Tensoren:\")\n",
    "print(output_details)\n",
    "\n",
    "input_shape = (1, 64, 64, 3)\n",
    "input_a_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "input_b_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], input_a_data)\n",
    "interpreter.set_tensor(input_details[1]['index'], input_b_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and Test it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for pair in pairs.astype(np.float32):\n",
    "    input_a_data = np.expand_dims(pair[0], axis=0)  \n",
    "    input_b_data = np.expand_dims(pair[1], axis=0)\n",
    "\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_a_data)\n",
    "    interpreter.set_tensor(input_details[1]['index'], input_b_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit: 0.894\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "binary_predictions = [1 if pred >= threshold else 0 for pred in predictions]\n",
    "\n",
    "correct_predictions = np.equal(binary_predictions, pair_labels)\n",
    "\n",
    "accuracy = np.mean(correct_predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
